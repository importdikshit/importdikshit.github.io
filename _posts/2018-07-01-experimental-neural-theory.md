---
layout: post
title: "On Neural Networks"
description: "Experimentally + Theoretically testing everything I have been told about Neural Networks"
date: 2018-07-01
comments: true
---

Have been handed a laundry list of heuristics for Neural Networks.
Now, I will systematically examine the theory and experiment with it.

TL;DR: Neural Network research is remarkably ad hoc.

---

## MNIST DATASET

- DATA NORMALIZATION AND STANDARDIZATION
  (and watching for standardization post and pre split - else  different representations)
  (also what is group sparsity)
- PCA and DECORRELATION ETC OF DATA + HOW IT HELPS REPRESENTATION WISE

- WEIGHT INITIALIZATIONS AND EFFECTS
- REGULARIZATION - L1, L2, etc. Max norm etc.
- STOCHASTIC REGULARIZATION IE DROPOUT BABY

- LEARNING RATE DECAY - STEP SIZE SHEBANG
  (and using EARLY STOPPING + WHY EARLY STOPPING WORKS)

- DIFFERENT AND CUSTOM LOSS FUNCTIONS
- DIFFERENT ACTIVATION FUNCTIONS (CONVEX FUNCTIONS VS NONCON?)
- DIFFERENT OPTIMIZERS, RUDER ARTICLE
- WTF IS BATCH NORMALIZATION + EFFECTS
- HYPERPARAMETER JAZZ - AND SELECTION METHODS
- MINIBATCH vs. STOCHASTIC LEARNING APPROACH (NOISE AS A REGULARIZER)
- BATCHSIZE AND GRADIENT DESCENT WITH VARYING BATCH SIZE VS EPOCHS
(Very large batch can reduce generlization ability according to https://arxiv.org/abs/1609.04836)

- VANISHING GRADIENT AND TRAINING *DEEP* NEURAL NETWORKS
- ADD MORE LAYERS - AND HOW NEURAL TOPOLOGY CHANGES SHIT
- TRANSFER LEARNING AND PRETRAINING
- DATA AUGMENTATION FOR ROBUSTNESS + ADVERSERIAL ATTACKS!

## CUSTOM DATASET FOR n=1 CASE EXPERIMENTS

- PCA and DECORRELATION ETC OF DATA + HOW IT HELPS REPRESENTATION WISE
- DOES REMOVING THE BIAS HAVE ANY EFFECT
- PCA INTO 2 DIMENSIONS AND THEN VIZ VS VIZ 2 DIMENSIONAL DATA TO CHECK
- GRADIENT DESCENT VS SEARCHING WEIGHT SPACES - C.o.DIMENSIONALITY
- TRYING RANDOM DATA TO CHECK FOR GENERALIZATION

- SHUFFLING YOUR DATASET AND HOW THAT IMPACTS TRAINING vs DETERMINISTIC ORDER
- UNIVERSAL APPROXIMATION THEOREMS & BREAKDOWN

- LOCLA MINIMA: DETERMINISTIC PARAMETERS FOR THE MNIST DATA
- THE WAY THE DECISION BOUNDARY OUTSIDE THE LEARNT SPACE FLUCTATES - KARPATHY LIKE EXPERIMENT
- CAN YOU PREDICT A STRUCTURE LIKE APPROXIMATING A VARIANCE MATRIX FOR SOME DATA DISTRIBUTION?
- IDENTITY FUNCTION SAVING IN PARAMETERS DEAL - FROM GOODFELLOW?

---

Also - check out as a reference (http://theorangeduck.com/page/neural-network-not-working) and the general template.



---
